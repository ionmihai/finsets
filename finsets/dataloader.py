# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_dataloader.ipynb.

# %% auto 0
__all__ = ['fetch', 'batch_download', 'get_text_file_from_url']

# %% ../nbs/02_dataloader.ipynb 3
from typing import List
import requests
from io import StringIO
from importlib import import_module

import pandas as pd

from . import metadata 
from .storage import BaseStorage, LocalStorage
from . import PACKAGE_DIR

# %% ../nbs/02_dataloader.ipynb 5
def fetch(finsets_func: str, # Which finsets function creates the dataset we want; must include full path to function (e.g. 'wrds.compa.clean')
          storage: BaseStorage=None, # Storage object indicating where the data should be retrieved from / saved to
          save: bool=False, # If dataset needs to be downloaded, this specifies if it should be saved to `storage`
          force_fetch: bool=False, # Whether to redownload/reprocess the dataset even if it exists in storage
          extension: str='.pkl', # Extension that will be automatically suffixed to `finsets_func` to generate the dataset name 
          **func_kwargs # Arguments to be passed to `finsets_func`
          ):

    dataset_name = finsets_func + extension

    # Split `finsets_func` into module name and function name
    s = finsets_func.split('.')
    func = s[-1]
    module = ".".join(s[:-1])

    if storage.exists(dataset_name) and (not storage.is_stale(dataset_name)) and (not force_fetch):
        print(f"Dataset {dataset_name} loaded from storage.")
        df = storage.load(dataset_name)
    else:
        print(f"Building {dataset_name} using finsets.{module}.{func}()")
        func = getattr(import_module(f'finsets.{module}'), func)
        df = func(**func_kwargs)
        
        if save: storage.save(df, dataset_name)

    return df

# %% ../nbs/02_dataloader.ipynb 17
def batch_download(finsets_functions: List[str], #List of finsets functions that will generate the datasets we want
                   storage: BaseStorage,
                   **kwargs
                   ):

    for dset in finsets_functions:
        fetch(dset, save=True, storage=storage, force_fetch=True, **kwargs) 

# %% ../nbs/02_dataloader.ipynb 22
def get_text_file_from_url (url, #Data at this url must be readable with pandas.read_csv
             nrows: int=None, #Get only the first `nrows` from the file. If None, gets the entire file
             delimiter: str=',',
             **pd_read_csv_kwargs,
    ) -> pd.DataFrame:
    "Gets the first `nrows` from the file found at `url`. Data at `url` must be separated by `delimiter` and be readable by pandas.read_csv"
    
    if nrows is not None:
        response = requests.get(url, stream=True)
        response.raise_for_status()

        lines = []
        for i, line in enumerate(response.iter_lines(decode_unicode=True)):
            if i >= nrows: break
            lines.append(line)
        partial_csv = '\n'.join(lines)

        return pd.read_csv(StringIO(partial_csv), delimiter=delimiter, **pd_read_csv_kwargs)

    return pd.read_csv(url, delimiter=delimiter,  **pd_read_csv_kwargs)

